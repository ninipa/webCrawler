#!/usr/bin/python3
# -*- coding: utf-8 -*-

from time import time
from time import sleep
from queue import Queue
import threading
from random import randint

import requests
from lxml import etree

class Xiaohongshu():
    #class attribute, to control threads
    ID_CRAWLER_EXIT = False
    ITEM_CRAWLER_EXIT = False
    def __init__(self):
        # ====================== user define area --- begin ============================
        self.threadNum = 10
        self.homeUrl = "http://www.xiaohongshu.com/explore"
        self.tabUrl = "http://www.xiaohongshu.com/web_api/sns/v2/homefeed/notes"
        self.itemUrlPrefix = "http://www.xiaohongshu.com/discovery/item/"
        self.UA = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36"
        self.X_Forwarded_For = str(randint(1,255))+"."+str(randint(1,255))+"."+str(randint(1,255))+"."+str(randint(1,255))
        self.headers = {
            "User-Agent":self.UA,
            "Connection":"keep-alive",
            "Accept":"application/json, text/plain, */*",
            "Accept-Encoding":"gzip, deflate",
            "X-Forwarded-For":self.X_Forwarded_For,
            "Accept-Language":"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,zh-TW;q=0.6"
        }
        self.maxPageNum = 40
        self.likesThreshold = 1000 #only items whose "likes" more than threshold will be put in the itemLinkQueue
        #Query Strings
        self.pageSize = 50  #This is a value got by manual testing; make each request get as much info
        self.tab = [
            'recommend',
            #'fasion',
            #'cosmetics',
            #'food',
            #'sport',
            #'media',
            #'travel',
            #'home',
            #'babycare',
            #'books',
            #'digital',
            #'mens_fasion',
            #'medicine',
        ]
        self.pageXpath = [
            #text body [0]
            '//div[@class="card-note pc-container"]/div[@class="left-card"]/div[not(contains(@class, "all-tip"))]/div[@class="content"]/p/text()',
            #likes [1]
            '//div[@class="card-note pc-container"]/div[@class="left-card"]/div[contains(@class, "tags")]//span[@class="like"]//text()',
            #comments [2]
            '//div[@class="card-note pc-container"]/div[@class="left-card"]/div[contains(@class, "tags")]//span[@class="comment"]//text()',
            #favorite [3]
            '//div[@class="card-note pc-container"]/div[@class="left-card"]/div[contains(@class, "tags")]//span[@class="star"]//text()',
            #time(list result) [4]
            '//div[@class="card-note pc-container"]/div[@class="left-card"]/div[contains(@class, "tags")]//span[@class="title" or @class="time"]//text()',
            #author [5]
            '//div[@class="card-note pc-container"]/div[@class="right-card"]//span[@class="name-detail"]/text()',
            #card-info(list result) [6]
            '//div[@class="card-note pc-container"]/div[@class="right-card"]//div[@class="card-info"]//span/text()',
            #image link(list result) [7]
            '//div[@class="card-note pc-container"]/div[@class="left-card"]/div[@class="bottom-gap"]//i[@class="img"]/@style',
        ]
        # ====================== user define area --- end ============================
        #create pageNum queue
        self.pageNumQueue = Queue()
        for i in range(1, self.maxPageNum+1):
            self.pageNumQueue.put(i)
        #link queue which stores link(ID) of each item
        self.itemLinkQueue = Queue()
        #item queue which stores all the info/content of each item
        self.itemInfoQueue = Queue()
        #dict equivlent to itemInfoQueue
        self.infoDict = {}

    def spider(self):
        '''
        Main function
        Step 1 --- get_item_link() according to Tabs, put item ID(link) into itemLinkQueue
        Step 2 --- crawl_item(), get likes, favorite, author info - {notes, fans, total-likes}, text, image
        Step 3 --- data_process(), sorted by "likes"
        '''
        # Step 1
        for tab in self.tab:
            self.get_item_link(tab)
        # Step 2
        print("itemLinkQueue size = %s"%self.itemLinkQueue.qsize())
        self.crawl_item()
        #while not self.itemInfoQueue.empty():
        #    print(self.itemInfoQueue.get(False))
        # Step 3
        self.data_process()

    def get_item_link(self, tab):
        '''
        Get ID of each link (http address)
        Multi-Threads
        Get "self.pageSize" items per request
        '''
        print("------ Multi-Thread Item ID Crawlers Start ------")
        #Get 1st page of the tab and cookies
        queryString = {
            'tab' : tab
        }
        self.session = requests.session()
        response = self.session.get(self.homeUrl, headers=self.headers, params=queryString, verify=False)
        cookies = self.session.cookies
        # -------- Multiple Threads control begin ------------
        crawlerList = []
        for i in range(0, self.threadNum):
            crawlerList.append("webCrawler_NO.%s"%i)
        #print(crawlerList)
        threadPool = []
        for crawlerName in crawlerList:
            thread = ThreadGetItemLink(self.pageNumQueue, self.itemLinkQueue, crawlerName, self.tabUrl, tab, self.pageSize, self.headers, cookies, self.likesThreshold)
            thread.start()
            threadPool.append(thread)
        # keep threading until pageNumQueue is empty
        while not self.pageNumQueue.empty():
            pass
        # change flag to make threads end
        Xiaohongshu.ID_CRAWLER_EXIT = True
        print("pageNum queue is empty")
        #wait until threads ends
        for thread in threadPool:
            thread.join()
        print("------ Multi-Thread Item ID Crawlers Are Finished ------")
        # -------- Multiple Threads control end ------------

    def crawl_item(self):
        '''
        Get content of each link, including text body, likes/comment/favorite, publish time, author info, img link
        Put into itemInfoQueue
        '''
        print("------ Multi-Thread Item Content Crawlers Start ------")
        # -------- Multiple Threads control begin ------------
        crawlerList = []
        for i in range(0, self.threadNum):
            crawlerList.append("webCrawler_NO.%s"%i)
        #print(crawlerList)
        threadPool = []
        for crawlerName in crawlerList:
            thread = ThreadGetItemInfo(self.itemLinkQueue, self.itemInfoQueue, crawlerName, self.itemUrlPrefix, self.pageXpath, self.headers, self.session.cookies)
            thread.start()
            threadPool.append(thread)
        # keep threading until IDQueue is empty
        while not self.itemLinkQueue.empty():
            pass
        # change flag to make threads end
        Xiaohongshu.ITEM_CRAWLER_EXIT = True
        print("ID queue is empty")
        #wait until threads ends
        for thread in threadPool:
            thread.join()
        print("------ Multi-Thread Item Content Crawlers Are Finished ------")
        # -------- Multiple Threads control end ------------

    def data_process(self):
        while not self.itemInfoQueue.empty():
            item = self.itemInfoQueue.get(False)
            self.infoDict[item[0]] = item[1:]   #key=url, value=other_data
        #for ditem in self.infoDict:
        #    print([ditem,self.infoDict[ditem]])
        #sorted by "likes"
        orderedItem = sorted(self.infoDict.items(), key=lambda x:x[1][3], reverse=True)
        with open('item.txt', 'w') as wf:
            for i in orderedItem:
                print(i)
                wf.write(str(i)+'\n')


class ThreadGetItemLink(threading.Thread):
    def __init__(self, pageNumQueue, itemLinkQueue, threadName, tabUrl, tab, pageSize, headers, cookies, likesThreshold):
        self.pageNumQueue = pageNumQueue
        self.itemLinkQueue = itemLinkQueue
        self.threadName = threadName
        self.tabUrl = tabUrl
        self.tab = tab
        self.pageSize = pageSize
        self.headers = headers
        self.cookies = cookies
        self.likesThreshold = likesThreshold
        #call parent __init__() to initialize
        super(ThreadGetItemLink, self).__init__()

    def run(self):
        while not Xiaohongshu.ID_CRAWLER_EXIT:
            try:
                pageNum = self.pageNumQueue.get(False)
                print("processing pageNum=%s ThreamName=%s"%(pageNum,self.threadName))
                data = self.get_item_id(pageNum)
                if data == []:
                    #print("Invalid PageNum")
                    break
                else:
                    for item in data:
                        if item['likes'] >= self.likesThreshold:
                            #'likes' captured in tab is real number, 'likes' in each item page is probably "1.6ä¸‡" and not easy for sorting; so here we also store 'likes' in linkQueue
                            self.itemLinkQueue.put([item['id'],item['likes']])
            except:
                pass
        print("------ %s Ends ------"%self.threadName)

    def get_item_id(self, pageNum):
        queryString = {
            'page_size' : self.pageSize,
            'oid' : self.tab,
            'page' : pageNum
        }
        response = requests.get(self.tabUrl, headers=self.headers, cookies=self.cookies, params=queryString, verify=False)
        sleep(randint(5,10))
        data = response.json()['data']
        return data

class ThreadGetItemInfo(threading.Thread):
    def __init__(self, itemLinkQueue, itemInfoQueue, threadName, itemUrlPrefix, pageXpath, headers, cookies):
        self.itemLinkQueue = itemLinkQueue
        self.itemInfoQueue = itemInfoQueue
        self.threadName = threadName
        self.itemUrlPrefix = itemUrlPrefix
        self.pageXpath = pageXpath
        self.headers = headers
        self.cookies = cookies
        #call parent __init__() to initialize
        super(ThreadGetItemInfo, self).__init__()

    def run(self):
        while not Xiaohongshu.ITEM_CRAWLER_EXIT:
            try:
                itemLinkAndLikes = self.itemLinkQueue.get(False)
                itemUrl = self.itemUrlPrefix+itemLinkAndLikes[0]  #gen full link
                itemlikes = int(itemLinkAndLikes[1])
                print("%s processing %s"%(self.threadName, itemUrl))
                info = self.get_item_info(itemUrl, itemlikes)
                self.itemInfoQueue.put(info)
            except:
                pass
        print("------ %s Ends ------"%self.threadName)

    def get_item_info(self, url, likes):
        response = requests.get(url, headers=self.headers, cookies=self.cookies, verify=False)
        sleep(randint(5,10))
        pageSrc = response.text
        # --------- Xpath captures info ----------
        # ---- example of raw result ----
        # ['ç”·æœ‹å‹ç”Ÿæ—¥ä¸çŸ¥é“é€ä»€ä¹ˆï¼Ÿ', 'ç”·æœ‹å‹20å²ç”Ÿæ—¥ï½å‚è€ƒä¸€ä¸‹å§ ä¹°çš„ä¸œè¥¿ä»·æ ¼ä»·æ ¼éƒ½ä¸è´µ ä½†æ˜¯èŠ±çš„æ—¶é—´é•¿ è€—æ—¶å·®ä¸å¤šä¸€ä¸ªåŠæœˆä¸¤ä¸ªæœˆå·¦å³', 'ç¤¼ç‰©ğŸä½†æ˜¯æ²¡æœ‰é€‰æ‹©ä»€ä¹ˆ1-20å²çš„ç¤¼ç‰© æˆ‘çœ‹å¾ˆå¤šå¥³ç”Ÿä¼šå‡†å¤‡ç”·æœ‹å‹å‡ºç”Ÿåˆ°ç°åœ¨æ¯ä¸€å¹´çš„ç¤¼ç‰© æ˜¯å¾ˆæµªæ¼«æ‹‰ï¼ä½†æ˜¯å¥¶ç“¶å°è¡£æœå°ç©å…·é‚£äº›çœŸçš„æŒºä¸å®ç”¨çš„ æˆ‘å°±æŒ‘äº†ä¸€äº›ç”·æœ‹å‹æ¯”è¾ƒéœ€è¦çš„å°ä¸œè¥¿ è¿™æ ·ä¹Ÿä¸ä¼šæµªè´¹é’± ä»–è¿˜èƒ½ç”¨ ç¤¼ç‰©æ¸…å•åœ¨ç¬¬å…«ä¹å¼ å›¾ ã€‚å› ä¸ºèº²èº²è—è—çš„ç»™ä»–å‡†å¤‡ç¤¼ç‰©ä»–è¿˜æ˜¯æœ‰äº›å‘è§‰ æœ‰ç‚¹ç¤¼ç‰©è¿˜æ²¡åŒ… åˆšä¹°å°±è¢«ä»–è‡ªå·±æ‹¿å»ç”¨äº† å¤ªè¿‡åˆ†äº† è‡ªå·±diyçš„ç¤¼ç‰©å‰ä¸¤ç¯‡å°çº¢ä¹¦éƒ½æœ‰å†™è¿‡å•¦ï¼ç¤¼ç‰©åŒ…è£…çº¸æ˜¯åœ¨æ·˜å®çš„é²œèŠ±åº—åŒ…è£…ä¹°çš„ åå—çš„20å¤§å¼ ï¼ä¸å¸¦äº”å—ä¸€å¤§å·ï¼åŒ…20ä¸ªç¤¼ç‰©åˆšåˆšå¥½ï¼ä½†æ˜¯æœ€ååŒ…å¤§ç¤¼ç‰©ç›’éƒ½çš„æ—¶å€™è¿˜æ˜¯ä¸å¤Ÿç”¨ å°±é›¶æ—¶åˆä¹°äº† å¤§å®¶è¦è®¡ç®—å¥½ ç”¨çº¸ å¤§ç¤¼ç‰©çš„åŒ…è£…ç›’æ˜¯ç½‘ä¸Šä¹°çš„çº¸çš®ç®± çœ‹äº†å¾ˆå¤šç¤¼ç‰©ç›’éƒ½å¤ªè´µå¤ªå°äº† åæ­£åŒ…äº†åŒ…è£…çº¸éƒ½å¾ˆå¥½çœ‹ åŒ…è£…ç›’çš„å¤§å°æ˜¯45x45çš„ ä»·æ ¼æ‰8å—å·¦å³ å¯ä»¥è¯´æ˜¯å¾ˆå®æƒ äº† ç¤¼ç‰©2000-3000å·¦å³', 'è›‹ç³•ğŸ‚ç„¶åå°±æ˜¯è›‹ç³• å…³é”®è¯åœ¨ç¬¬å…«å¼ å›¾ å¥½çœ‹ é¢œå€¼é«˜ é€‚åˆæ‹ç…§ æˆ‘ä¹Ÿæœ‰è€ƒè™‘è¿‡é‚£ç§è‰²å½©é²œè‰³çš„insè›‹ç³• ä½†æ˜¯æƒ³æƒ³ç”·ç”Ÿåº”è¯¥æ¯”è¾ƒå–œæ¬¢è¿™ç§æ±½è½¦çš„å§ï¼è½¦é’¥åŒ™é€ä¸èµ· æ¨¡å‹è½¦å°±å¯ä»¥ï¼ä¸‹æ¥220å·¦å³', 'å¸ƒç½®ğŸ æ°”çƒéƒ½éƒ½å¾ˆä¾¿å®œ æˆ‘ä¹°çš„æ˜¯ä¸€ä¸ªé±¼å°¾æ——å¥—é¤è¿˜æœ‰ä¸€ç™¾ä¸ªé“¶è‰²å’Œé»‘è‰²æ°”çƒ å°±æ˜¯æ°¢æ°”è´µ ä¸€ç“¶æ°¦æ°”å°±è¦100å— ä½†æ˜¯æ³¨æ„æ°¦æ°”ä¸è¦é ç«ï¼Œå¸ƒç½®å¤§æ¦‚ä¸¤ä¸ªå°æ—¶å·¦å³ èƒŒç€ç”·æœ‹å‹æå‰å…ˆåˆ°äº†é…’åº—å«å¥½å§å¦¹ä¸€èµ·å¸ƒç½®çš„ ç„¶åä»–åˆ°é…’åº—è¿˜ç”Ÿæ°”å› ä¸ºæˆ‘æ‰“æ°”çƒæ²¡æœ‰å›åˆ°ä»–ä¿¡æ¯ï¼Ÿåˆ°äº†é…’åº—å·å·ç¬‘ å› ä¸ºå‡†å¤‡äº†å¾ˆä¹… å¸Œæœ›ä»–èƒ½æ„ŸåŠ¨çš„æ‰çœ¼æ³ª ä½†æ˜¯æ²¡æœ‰ æƒ³æŠŠä»–æ‰“åˆ°æ‰çœ¼æ³ª ä»·æ ¼150-200å·¦å³', 'é…’åº—ğŸ¨é…’åº—åœ°å€æ˜¯å‡¯è´ä¸½åœ¨ç›ç”°æ²™å¤´è§’é‚£è¾¹ é™„è¿‘æœ‰å¾ˆå¤šå¥½åƒçš„å±±æ°´è‚ ç²‰ ç”Ÿç…åŒ… å»ä¸€å®šè¦åƒä¸€ä¸‹æ—è¾¹çš„ç”Ÿç…åŒ… çœŸçš„è¶…çº§å¥½åƒ ä½†æ˜¯å›¾ç‰‡æ”¾ä¸ä¸‹äº†é…’åº—æå‰ä¸€ä¸ªæœˆå®šäº† å› ä¸ºä»–ç”Ÿæ—¥åˆæ˜¯ç«¯åˆèŠ‚åˆæ˜¯çˆ¶äº²èŠ‚ å¹³æ—¶400-500çš„ä»·æ ¼åˆ°é‚£ä¸€å¤©åªå‰©1000å¤šçš„äº†ï¼ç„¶åå°±æ˜¯æ³³æ± å±±è·Ÿæµ·ï¼ä»·æ ¼668', 'å»é…’åº—çš„æ—¶å€™æ˜¯ä¸‹åˆå¤©æ°”è¶…çƒ­ç„¶åè¿˜ï¼æ‹‰ç€ä¸€ä¸ªè¶…å¤§çš„è¡Œæç®± æŠ±ç€45x45çš„çº¸çš®ç®± ç¤¼ç‰©è¡Œæç®±éƒ½è£…ä¸ä¸‹ç„¶åè¿˜æœ‰ä¸€å¤§ç½æ°¢æ°” æˆ‘è¿˜ç©¿ç€é«˜è·Ÿé‹ å¯ä»¥è¯´æ˜¯éå¸¸æ‹¼å‘½äº†ï¼ä½†æ˜¯ä»–å¼€å¿ƒå¹¸è‹¦ä¹Ÿå€¼å¾—äº†ï½ğŸ˜—']
        # ['2428']
        # ['313']
        # ['6417']
        # ['å‘å¸ƒäº', '2018-06-18 22:09']
        # ['é¸¡çäº²ä½ çš„å°è„¸']
        # ['ç¬”è®°', '8', 'ç²‰ä¸', '2208', 'è·èµä¸æ”¶è—', '1.9ä¸‡']
        # ['background-image:url(//ci.xiaohongshu.com/87ef3586-9267-4987-87ac-8924dbec039a@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/911a85e4-7a0d-4896-966e-6a21c1fbf645@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/27e54170-2188-4319-a6c9-234e87237070@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/b7f2a690-1415-491d-a9e5-2078d1222686@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/b83128e7-7714-403a-9e49-5c5b5adf2877@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/700c8395-b13c-4675-8792-2bfe770bd719@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/53fd82d0-19ea-4c54-94ee-8d09cb1f465f@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/6ac22237-0955-4674-9a06-fb2c749aff5f@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);', 'background-image:url(//ci.xiaohongshu.com/5f2dd225-d14b-4d09-8453-a3ae954b054d@r_750w_750h_ss1.jpg?imageView2/2/w/100/h/100/q/90);']
        html = etree.HTML(pageSrc)
        textBody = html.xpath(self.pageXpath[0])
        #likes = int(html.xpath(self.pageXpath[1])[0])
        comment = html.xpath(self.pageXpath[2])[0]
        favorite = html.xpath(self.pageXpath[3])[0]
        publishTime = html.xpath(self.pageXpath[4])
        author = html.xpath(self.pageXpath[5])
        cardInfo = html.xpath(self.pageXpath[6])
        imgRawLink = html.xpath(self.pageXpath[7])
        #create a list to store itemInfo
        info = [
            url,
            author,
            cardInfo,
            publishTime,
            likes,
            comment,
            favorite,
            textBody,
            imgRawLink,
        ]
        print("%s URL=%s"%(self.threadName, url))
        #print(textBody)
        #print(likes)
        #print(comment)
        #print(favorite)
        #print(publishTime)
        #print(author)
        #print(cardInfo)
        #print(imgRawLink)
        return info

if __name__ == '__main__':
    startTime = time()
    crawler = Xiaohongshu()
    crawler.spider()
    endTime = time()
    print("------ Time Cost = %.2f Seconds"%(endTime-startTime))
